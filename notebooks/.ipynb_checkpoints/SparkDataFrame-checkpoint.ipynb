{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e644a77-71fa-4070-b5af-38bdc08c987f",
   "metadata": {},
   "source": [
    "## Spark DataFrame Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1958014-2aa8-4bd2-b144-25e357eff6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1284f8-11a5-49d2-bace-d3dd2720f3ed",
   "metadata": {},
   "source": [
    "### Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4b8d631-927e-4d13-bca2-3c5db0c9c6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/boran/Desktop/financial-ETL/data\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path().resolve().parent / \"data\" \n",
    "print(DATA_DIR)\n",
    "with open(DATA_DIR / 'stock_data.json', 'r') as file:\n",
    "    # Load the data from the file\n",
    "    nested_data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de0949-b084-4a8b-96a5-c28df264df5f",
   "metadata": {},
   "source": [
    "### Create Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26c5f672-f7fa-4ad1-8f85-3e405867ddc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m sbux_data \u001b[38;5;241m=\u001b[39m nested_data[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAAPL\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create an empty DataFrame\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mcreateDataFrame([], schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Flatten each key and add as columns to the DataFrame\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, nested_dict \u001b[38;5;129;01min\u001b[39;00m sbux_data\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract data for 'AAPL'\n",
    "sbux_data = nested_data[1]['AAPL']\n",
    "\n",
    "# Create an empty DataFrame\n",
    "df = spark.createDataFrame([], schema=None)\n",
    "\n",
    "# Flatten each key and add as columns to the DataFrame\n",
    "for key, nested_dict in sbux_data.items():\n",
    "    temp_df = spark.createDataFrame([(k, v) for k, v in nested_dict.items()], schema=[\"Date\", key])\n",
    "    if df.rdd.isEmpty():\n",
    "        df = temp_df\n",
    "    else:\n",
    "        df = df.join(temp_df, on=\"Date\", how=\"outer\")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c2c8a-f784-48df-8243-c87bf66fdb5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f1d5e0-e5ad-44e4-bf55-75f16dbab748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1b4695-df58-4f5f-91f2-28810205db87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61300ddc-465a-441b-b4bd-1cf18abe1234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af3582-a289-4b13-950e-52f8d2154bec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df2e287-3405-4da6-9224-f87e3abcdee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369b2449-f2cf-4eaf-a0cf-781a7827628e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1934c032-cacc-4513-a94e-65dc1167e6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
